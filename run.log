(proj2) [suddapal@v027 output]$ python ../src/main.py 
SimpleANN(
  (fc1): Linear(in_features=784, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=32, bias=True)
  (dropout2): Dropout(p=0.25, inplace=False)
  (fc3): Linear(in_features=32, out_features=10, bias=True)
)

Starting training with Optimizer: Adam
Epoch [1/20] - Train Loss: 0.6800, Train Acc: 78.85%, Val Loss: 0.3462, Val Acc: 89.58%
Epoch [2/20] - Train Loss: 0.4317, Train Acc: 87.47%, Val Loss: 0.3464, Val Acc: 90.24%
Epoch [3/20] - Train Loss: 0.3913, Train Acc: 88.87%, Val Loss: 0.3424, Val Acc: 90.34%
Epoch [4/20] - Train Loss: 0.3799, Train Acc: 89.18%, Val Loss: 0.3748, Val Acc: 89.47%
Epoch [5/20] - Train Loss: 0.3602, Train Acc: 89.99%, Val Loss: 0.3051, Val Acc: 91.68%
Epoch [6/20] - Train Loss: 0.3503, Train Acc: 90.08%, Val Loss: 0.3042, Val Acc: 90.94%
Epoch [7/20] - Train Loss: 0.3459, Train Acc: 90.32%, Val Loss: 0.3136, Val Acc: 90.92%
Epoch 00008: reducing learning rate of group 0 to 5.0000e-03.
Epoch [8/20] - Train Loss: 0.3444, Train Acc: 90.57%, Val Loss: 0.3833, Val Acc: 89.69%
Epoch [9/20] - Train Loss: 0.2682, Train Acc: 92.52%, Val Loss: 0.2547, Val Acc: 92.95%
Epoch [10/20] - Train Loss: 0.2609, Train Acc: 92.71%, Val Loss: 0.2601, Val Acc: 92.97%
Epoch 00011: reducing learning rate of group 0 to 2.5000e-03.
Epoch [11/20] - Train Loss: 0.2607, Train Acc: 92.77%, Val Loss: 0.2752, Val Acc: 92.74%
Epoch [12/20] - Train Loss: 0.2270, Train Acc: 93.62%, Val Loss: 0.2261, Val Acc: 93.99%
Epoch [13/20] - Train Loss: 0.2213, Train Acc: 93.84%, Val Loss: 0.2397, Val Acc: 93.88%
Epoch 00014: reducing learning rate of group 0 to 1.2500e-03.
Epoch [14/20] - Train Loss: 0.2176, Train Acc: 93.94%, Val Loss: 0.2276, Val Acc: 94.04%
Epoch [15/20] - Train Loss: 0.2059, Train Acc: 94.20%, Val Loss: 0.2133, Val Acc: 94.32%
Epoch [16/20] - Train Loss: 0.1995, Train Acc: 94.38%, Val Loss: 0.2149, Val Acc: 94.27%
Epoch [17/20] - Train Loss: 0.1970, Train Acc: 94.39%, Val Loss: 0.2132, Val Acc: 94.37%
Epoch [18/20] - Train Loss: 0.1954, Train Acc: 94.46%, Val Loss: 0.2140, Val Acc: 94.41%
Epoch [19/20] - Train Loss: 0.1954, Train Acc: 94.48%, Val Loss: 0.2115, Val Acc: 94.57%
Epoch [20/20] - Train Loss: 0.1932, Train Acc: 94.45%, Val Loss: 0.2242, Val Acc: 93.81%

Testing with Optimizer: Adam
Test Loss: 0.1916 | Test Accuracy: 94.76%
SimpleANN(
  (fc1): Linear(in_features=784, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=32, bias=True)
  (dropout2): Dropout(p=0.25, inplace=False)
  (fc3): Linear(in_features=32, out_features=10, bias=True)
)

Starting training with Optimizer: SGD
Epoch [1/20] - Train Loss: 1.3812, Train Acc: 58.30%, Val Loss: 0.6124, Val Acc: 84.99%
Epoch [2/20] - Train Loss: 0.6322, Train Acc: 80.97%, Val Loss: 0.4347, Val Acc: 88.19%
Epoch [3/20] - Train Loss: 0.5040, Train Acc: 84.94%, Val Loss: 0.3591, Val Acc: 90.13%
Epoch [4/20] - Train Loss: 0.4468, Train Acc: 86.83%, Val Loss: 0.3419, Val Acc: 90.24%
Epoch [5/20] - Train Loss: 0.4139, Train Acc: 87.81%, Val Loss: 0.3068, Val Acc: 91.17%
Epoch [6/20] - Train Loss: 0.3790, Train Acc: 89.03%, Val Loss: 0.2749, Val Acc: 92.24%
Epoch [7/20] - Train Loss: 0.3549, Train Acc: 89.75%, Val Loss: 0.2635, Val Acc: 92.43%
Epoch [8/20] - Train Loss: 0.3330, Train Acc: 90.26%, Val Loss: 0.2431, Val Acc: 92.98%
Epoch [9/20] - Train Loss: 0.3115, Train Acc: 90.95%, Val Loss: 0.2311, Val Acc: 93.18%
Epoch [10/20] - Train Loss: 0.2930, Train Acc: 91.39%, Val Loss: 0.2207, Val Acc: 93.56%
Epoch [11/20] - Train Loss: 0.2801, Train Acc: 91.88%, Val Loss: 0.2040, Val Acc: 93.92%
Epoch [12/20] - Train Loss: 0.2640, Train Acc: 92.32%, Val Loss: 0.2101, Val Acc: 93.78%
Epoch [13/20] - Train Loss: 0.2502, Train Acc: 92.76%, Val Loss: 0.1959, Val Acc: 94.38%
Epoch [14/20] - Train Loss: 0.2401, Train Acc: 92.99%, Val Loss: 0.1779, Val Acc: 94.80%
Epoch [15/20] - Train Loss: 0.2306, Train Acc: 93.24%, Val Loss: 0.1765, Val Acc: 95.04%
Epoch [16/20] - Train Loss: 0.2210, Train Acc: 93.63%, Val Loss: 0.1716, Val Acc: 95.06%
Epoch [17/20] - Train Loss: 0.2136, Train Acc: 93.71%, Val Loss: 0.1698, Val Acc: 94.91%
Epoch [18/20] - Train Loss: 0.2053, Train Acc: 94.03%, Val Loss: 0.1572, Val Acc: 95.41%
Epoch [19/20] - Train Loss: 0.1978, Train Acc: 94.19%, Val Loss: 0.1535, Val Acc: 95.62%
Epoch [20/20] - Train Loss: 0.1926, Train Acc: 94.35%, Val Loss: 0.1490, Val Acc: 95.56%

Testing with Optimizer: SGD
Test Loss: 0.1408 | Test Accuracy: 95.76%
SimpleANN(
  (fc1): Linear(in_features=784, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=32, bias=True)
  (dropout2): Dropout(p=0.25, inplace=False)
  (fc3): Linear(in_features=32, out_features=10, bias=True)
)

Starting training with Optimizer: RMSprop
Epoch [1/20] - Train Loss: 1.1308, Train Acc: 68.43%, Val Loss: 1.1173, Val Acc: 65.64%
Epoch [2/20] - Train Loss: 0.5499, Train Acc: 83.92%, Val Loss: 0.9609, Val Acc: 73.82%
Epoch [3/20] - Train Loss: 0.4816, Train Acc: 86.27%, Val Loss: 0.5793, Val Acc: 83.77%
Epoch [4/20] - Train Loss: 0.4483, Train Acc: 87.36%, Val Loss: 0.8536, Val Acc: 79.04%
Epoch [5/20] - Train Loss: 0.4427, Train Acc: 87.42%, Val Loss: 0.3757, Val Acc: 89.42%
Epoch [6/20] - Train Loss: 0.4214, Train Acc: 88.16%, Val Loss: 1.4829, Val Acc: 72.77%
Epoch 00007: reducing learning rate of group 0 to 5.0000e-03.
Epoch [7/20] - Train Loss: 0.4115, Train Acc: 88.37%, Val Loss: 0.4437, Val Acc: 90.04%
Epoch [8/20] - Train Loss: 0.3168, Train Acc: 91.24%, Val Loss: 0.6068, Val Acc: 84.19%
Epoch [9/20] - Train Loss: 0.3096, Train Acc: 91.21%, Val Loss: 0.2972, Val Acc: 91.12%
Epoch [10/20] - Train Loss: 0.3023, Train Acc: 91.51%, Val Loss: 0.2577, Val Acc: 93.00%
Epoch [11/20] - Train Loss: 0.2921, Train Acc: 91.82%, Val Loss: 0.2920, Val Acc: 92.03%
Epoch 00012: reducing learning rate of group 0 to 2.5000e-03.
Epoch [12/20] - Train Loss: 0.2915, Train Acc: 91.90%, Val Loss: 0.2710, Val Acc: 92.97%
Epoch [13/20] - Train Loss: 0.2499, Train Acc: 92.84%, Val Loss: 0.2431, Val Acc: 93.27%
Epoch [14/20] - Train Loss: 0.2406, Train Acc: 93.03%, Val Loss: 0.2560, Val Acc: 93.49%
Epoch [15/20] - Train Loss: 0.2415, Train Acc: 93.00%, Val Loss: 0.2410, Val Acc: 93.64%
Epoch [16/20] - Train Loss: 0.2344, Train Acc: 93.24%, Val Loss: 0.2628, Val Acc: 93.40%
Epoch 00017: reducing learning rate of group 0 to 1.2500e-03.
Epoch [17/20] - Train Loss: 0.2344, Train Acc: 93.30%, Val Loss: 0.2710, Val Acc: 93.06%
Epoch [18/20] - Train Loss: 0.2183, Train Acc: 93.67%, Val Loss: 0.2260, Val Acc: 94.21%
Epoch [19/20] - Train Loss: 0.2144, Train Acc: 93.91%, Val Loss: 0.2321, Val Acc: 94.25%
Epoch 00020: reducing learning rate of group 0 to 6.2500e-04.
Epoch [20/20] - Train Loss: 0.2134, Train Acc: 93.81%, Val Loss: 0.2362, Val Acc: 94.07%

Testing with Optimizer: RMSprop
Test Loss: 0.2104 | Test Accuracy: 94.47%
SimpleANN(
  (fc1): Linear(in_features=784, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=32, bias=True)
  (dropout2): Dropout(p=0.25, inplace=False)
  (fc3): Linear(in_features=32, out_features=10, bias=True)
)

Starting training with Optimizer: AdamW
Epoch [1/20] - Train Loss: 0.7644, Train Acc: 74.72%, Val Loss: 0.3369, Val Acc: 90.13%
Epoch [2/20] - Train Loss: 0.4285, Train Acc: 87.88%, Val Loss: 0.3270, Val Acc: 90.67%
Epoch [3/20] - Train Loss: 0.3929, Train Acc: 88.91%, Val Loss: 0.3249, Val Acc: 90.64%
Epoch [4/20] - Train Loss: 0.3895, Train Acc: 89.28%, Val Loss: 0.3232, Val Acc: 91.21%
Epoch [5/20] - Train Loss: 0.3729, Train Acc: 89.76%, Val Loss: 0.2860, Val Acc: 92.39%
Epoch [6/20] - Train Loss: 0.3609, Train Acc: 90.02%, Val Loss: 0.3190, Val Acc: 91.71%
Epoch 00007: reducing learning rate of group 0 to 5.0000e-03.
Epoch [7/20] - Train Loss: 0.3647, Train Acc: 89.82%, Val Loss: 0.3233, Val Acc: 91.24%
Epoch [8/20] - Train Loss: 0.2958, Train Acc: 91.96%, Val Loss: 0.2617, Val Acc: 92.61%
Epoch [9/20] - Train Loss: 0.2869, Train Acc: 92.12%, Val Loss: 0.2313, Val Acc: 93.53%
Epoch [10/20] - Train Loss: 0.2808, Train Acc: 92.37%, Val Loss: 0.2495, Val Acc: 93.00%
Epoch 00011: reducing learning rate of group 0 to 2.5000e-03.
Epoch [11/20] - Train Loss: 0.2736, Train Acc: 92.54%, Val Loss: 0.2728, Val Acc: 92.57%
Epoch [12/20] - Train Loss: 0.2455, Train Acc: 93.24%, Val Loss: 0.2172, Val Acc: 93.99%
Epoch [13/20] - Train Loss: 0.2347, Train Acc: 93.47%, Val Loss: 0.2198, Val Acc: 93.75%
Epoch 00014: reducing learning rate of group 0 to 1.2500e-03.
Epoch [14/20] - Train Loss: 0.2349, Train Acc: 93.38%, Val Loss: 0.2174, Val Acc: 94.10%
Epoch [15/20] - Train Loss: 0.2153, Train Acc: 93.99%, Val Loss: 0.2131, Val Acc: 94.23%
Epoch [16/20] - Train Loss: 0.2120, Train Acc: 94.00%, Val Loss: 0.2086, Val Acc: 94.32%
Epoch [17/20] - Train Loss: 0.2088, Train Acc: 94.03%, Val Loss: 0.2135, Val Acc: 94.16%
Epoch 00018: reducing learning rate of group 0 to 6.2500e-04.
Epoch [18/20] - Train Loss: 0.2094, Train Acc: 94.09%, Val Loss: 0.2129, Val Acc: 94.22%
Epoch [19/20] - Train Loss: 0.2002, Train Acc: 94.33%, Val Loss: 0.2102, Val Acc: 94.50%
Epoch 00020: reducing learning rate of group 0 to 3.1250e-04.
Epoch [20/20] - Train Loss: 0.1969, Train Acc: 94.45%, Val Loss: 0.2123, Val Acc: 94.25%

Testing with Optimizer: AdamW
Test Loss: 0.1957 | Test Accuracy: 94.41%
SimpleANN(
  (fc1): Linear(in_features=784, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=32, bias=True)
  (dropout2): Dropout(p=0.25, inplace=False)
  (fc3): Linear(in_features=32, out_features=10, bias=True)
)

Starting training with Optimizer: AdaGrad
Epoch [1/20] - Train Loss: 0.6979, Train Acc: 77.64%, Val Loss: 0.3613, Val Acc: 90.00%
Epoch [2/20] - Train Loss: 0.4377, Train Acc: 87.08%, Val Loss: 0.3109, Val Acc: 91.01%
Epoch [3/20] - Train Loss: 0.3902, Train Acc: 88.45%, Val Loss: 0.2854, Val Acc: 91.81%
Epoch [4/20] - Train Loss: 0.3672, Train Acc: 89.24%, Val Loss: 0.2659, Val Acc: 92.62%
Epoch [5/20] - Train Loss: 0.3466, Train Acc: 89.89%, Val Loss: 0.2547, Val Acc: 92.77%
Epoch [6/20] - Train Loss: 0.3305, Train Acc: 90.33%, Val Loss: 0.2480, Val Acc: 92.92%
Epoch [7/20] - Train Loss: 0.3178, Train Acc: 90.83%, Val Loss: 0.2338, Val Acc: 93.35%
Epoch [8/20] - Train Loss: 0.3079, Train Acc: 91.17%, Val Loss: 0.2305, Val Acc: 93.22%
Epoch [9/20] - Train Loss: 0.2989, Train Acc: 91.31%, Val Loss: 0.2249, Val Acc: 93.51%
Epoch [10/20] - Train Loss: 0.2915, Train Acc: 91.58%, Val Loss: 0.2158, Val Acc: 93.68%
Epoch [11/20] - Train Loss: 0.2813, Train Acc: 91.74%, Val Loss: 0.2123, Val Acc: 93.70%
Epoch [12/20] - Train Loss: 0.2784, Train Acc: 91.94%, Val Loss: 0.2130, Val Acc: 93.90%
Epoch [13/20] - Train Loss: 0.2708, Train Acc: 92.11%, Val Loss: 0.2015, Val Acc: 94.10%
Epoch [14/20] - Train Loss: 0.2637, Train Acc: 92.32%, Val Loss: 0.1998, Val Acc: 94.13%
Epoch [15/20] - Train Loss: 0.2545, Train Acc: 92.62%, Val Loss: 0.1934, Val Acc: 94.37%
Epoch [16/20] - Train Loss: 0.2518, Train Acc: 92.70%, Val Loss: 0.1919, Val Acc: 94.37%
Epoch [17/20] - Train Loss: 0.2465, Train Acc: 92.84%, Val Loss: 0.1890, Val Acc: 94.41%
Epoch [18/20] - Train Loss: 0.2439, Train Acc: 92.90%, Val Loss: 0.1871, Val Acc: 94.39%
Epoch [19/20] - Train Loss: 0.2392, Train Acc: 93.04%, Val Loss: 0.1858, Val Acc: 94.42%
Epoch [20/20] - Train Loss: 0.2335, Train Acc: 93.16%, Val Loss: 0.1824, Val Acc: 94.58%

Testing with Optimizer: AdaGrad
Test Loss: 0.1728 | Test Accuracy: 94.73%


